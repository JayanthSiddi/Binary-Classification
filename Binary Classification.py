# -*- coding: utf-8 -*-
"""Applied AI

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xCKIMo1wmFWExYHOvmtih0F3wOMdhoJY

Indicate the imported packages/libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
import seaborn as sns
from sklearn import tree
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier

"""Load the dataset and print the data information"""

file = pd.read_csv("dataset_assignment1.csv")
file.head()
file.shape

"""Understand the dataset
Print out the number of samples for each class in the dataset
"""

f1 = file["feature1"].values
f2 = file["feature2"].values
f3 = file["feature3"].values
f4 = file["feature4"].values
f5 = file["feature5"].values
f6 = file["feature6"].values
f7 = file["feature7"].values
f8 = file["feature8"].values
f9 = file["feature9"].values
c = file["class"].values


print(type(f1), type(f2), type(f3),type(f5),type(f6), type(f7), type(f8), type(f9), type(c))
print(f1.shape, f2.shape, f3.shape, f4.shape, f5.shape, f6.shape, f7.shape, f8.shape, f9.shape, c.shape)
file["class"].value_counts()

file.info()

file[file['class']==0].describe()

"""Plot some figures to visualize the dataset (e.g., histogram, etc.)"""

file[file['class']==1].describe()

file['feature1'].plot(kind='hist',title="Feature 1 Frequency Plot")
plt.show()

file['class'].value_counts().plot(kind='bar',title="Class Frequency Plot")
plt.show()

file['feature2'].plot(kind='hist',title="Feature 2 Frequency Plot")
plt.show()

file['feature3'].plot(kind='hist',title="Feature 3 Frequency Plot")
plt.show()

file['feature4'].plot(kind='hist',title="Feature 4 Frequency Plot")
plt.show()

file['feature5'].plot(kind='hist',title="Feature 5 Frequency Plot")
plt.show()

file['feature6'].plot(kind='hist',title="Feature 6 Frequency Plot")
plt.show()

file['feature7'].plot(kind='hist',title="Feature 7 Frequency Plot")
plt.show()

file['feature8'].plot(kind='hist',title="Feature 8 Frequency Plot")
plt.show()

file['feature9'].plot(kind='hist',title="Feature 9 Frequency Plot")
plt.show()

"""For each class, print out the statistical description of features (e.g., the input variable x), such as mean, std, max and min values, etc."""

description = file.describe()

# print the statistical description
print(description)
# calculate the median for each numeric column in the dataset
median = file.median()

# calculate the mode for each column in the dataset
mode = file.mode()

# print the median and mode
print("Median:\n", median)
print("\nMode:\n", mode)

"""Split data into a training dataset and a testing dataset (i.e., 80% v.s. 20%)"""

# Import the module
from sklearn.model_selection import train_test_split 

X = file[["feature1", "feature2", "feature3", "feature4", "feature5", "feature6", "feature7", "feature8", "feature9"]].values
y =file["class"].values
print(X)
print(y)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dt_c = DecisionTreeClassifier(criterion='gini' ,splitter='best', max_depth=None).fit(X_train,y_train)
print(f"Training Score : {dt_c.score(X_train,y_train)}")
print(f"Testing score : {dt_c.score(X_test,y_test)}")
Y_pred = dt_c.predict(X_test)

sns.heatmap(confusion_matrix(y_test,Y_pred),annot=True,cmap='OrRd')
plt.show()

from sklearn.metrics.cluster import entropy
from sklearn.tree import DecisionTreeClassifier
dt_c = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None).fit(X_train,y_train)
print(f"Training Score : {dt_c.score(X_train,y_train)}")
print(f"Testing score : {dt_c.score(X_test,y_test)}")
Y_pred = dt_c.predict(X_test)

sns.heatmap(confusion_matrix(y_test,Y_pred),annot=True,cmap='OrRd')
plt.show()

from sklearn.metrics.cluster import entropy
from sklearn.tree import DecisionTreeClassifier
dt_c = DecisionTreeClassifier(criterion='log_loss', splitter='best', max_depth=None).fit(X_train,y_train)
print(f"Training Score : {dt_c.score(X_train,y_train)}")
print(f"Testing score : {dt_c.score(X_test,y_test)}")
Y_pred = dt_c.predict(X_test)

sns.heatmap(confusion_matrix(y_test,Y_pred),annot=True,cmap='OrRd')
plt.show()

"""DT performs well using Gini Index"""

# KFold Cross Validation approach
kf = KFold(n_splits=5,shuffle=True)
kf.split(X_train)
print(X_train)

# Iterate over each train-test split
accuracy_model = []
for train_index, test_index in kf.split(file):
    # Split train-test
    A_train, A_test = file.iloc[train_index], file.iloc[test_index]
    b_train, b_test = y[train_index], y[test_index]
    # Train the model
    model = dt_c.fit(A_train, b_train)
    # Append to accuracy_model the accuracy of the model
    accuracy_model.append(accuracy_score(b_test, model.predict(A_test), normalize=True)*100)

# Print the accuracy    
print(accuracy_model)

dt=DecisionTreeClassifier()
dt=dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)

#classification Report
print(f"classification Report \n{classification_report(y_test,y_pred)}")

#confusion matrix
print(f"Confusion Matrix {confusion_matrix(y_test,y_pred)}")

sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,cmap='OrRd')
plt.show()

"""KNN"""

train_accuracies = {}
test_accuracies = {}
neighbors = np.arange(1, 5)
for neighbor in neighbors:    
  knn = KNeighborsClassifier(n_neighbors=neighbor, weights='uniform', algorithm='auto', metric='euclidean')    
  knn.fit(X_train, y_train)    
  train_accuracies[neighbor] = knn.score(X_train, y_train)    
  test_accuracies[neighbor] = knn.score(X_test, y_test)
  print(train_accuracies)
  print(test_accuracies)

"""**K = 3 gives highest accuracy = 94 %  So the best value of K is 3** """



knn2= KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='auto', metric='euclidean')
knn2.fit(X_train, y_train)
y_pred = knn2.predict(X_test)

#classification Report
print(f"classification Report \n{classification_report(y_test,y_pred)}")

#confusion matrix
print(f"Confusion Matrix {confusion_matrix(y_test,y_pred)}")

sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,cmap='OrRd')
plt.show()

# Iterate over each train-test split
accuracy_model = []
for train_index, test_index in kf.split(file):
    # Split train-test
    X_train, X_test = file.iloc[train_index], file.iloc[test_index]
    y_train, y_test = y[train_index], y[test_index]
    # Train the model
    model = knn.fit(X_train, y_train)
    # Append to accuracy_model the accuracy of the model
    accuracy_model.append(accuracy_score(y_test, model.predict(X_test), normalize=True)*100)

# Print the accuracy    
print(accuracy_model)

"""RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
rf_model = RandomForestClassifier(n_estimators=120).fit(X_train,y_train)
rf_model.score(X_train,y_train)
rf_model.score(X_test,y_test)
Y_pred = rf_model.predict(X_test)
print(Y_pred)
#Accuraccuracy_score
print(accuracy_score(y_test,Y_pred))

#confusion matrix
print(f"Confusion Matrix {confusion_matrix(y_test,Y_pred)}")

#classification Report
print(f"classification Report \n{classification_report(y_test,Y_pred)}")

sns.heatmap(confusion_matrix(y_test,Y_pred),annot=True,cmap='OrRd')
plt.show()

"""## Conclusion
1. The models are built using all 10 - attributes
2.Decision Tree algorithm had show really good results


"""

